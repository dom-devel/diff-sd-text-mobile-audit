{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find differences in primary page elements for mobile audits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several important checks you should compare when getting ready for the mobile first index., that have been explicitly touched on again and again. [Here's Gary tweeting about it](https://twitter.com/methode/status/904579213616918528). \n",
    "\n",
    "This script will help you find important differences in:\n",
    "- Text\n",
    "- Structured data \n",
    "\n",
    "It is worth reading the blog post that accompanies this workbook. You can find that here.\n",
    "\n",
    "**What will you need:**\n",
    "- A crawl which covers both your site and mobile version of your site, for each desktop page you'll need a mobile equivalent (or the comparison obviously won't happen.)\n",
    "- For each of those pages you'll need an extraction from that crawl of the full HTML for each page.\n",
    "\n",
    "In the example below I've used Screaming Frog.\n",
    "\n",
    "**How to use this:**\n",
    "\n",
    "Any time you see **Instructions**, you'll need to take an action in the cell below. Any time you see **Notes**, you just need to run the cells, if you're looking to modify the workbook then the notes may help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib\n",
    "import re\n",
    "import copy\n",
    "import json\n",
    "from deepdiff import DeepDiff\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import numpy as np\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import time\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "from tqdm import tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** First we load in the dataframe of pages to be compared. Because I'm using a Screaming frog upload in this example, we skip the first row.\n",
    "\n",
    "You could quite happily write some extra python to crawl all the pages here, but why build something where there already exists a great crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For all you Windows users out there you'll need to double escape your slashes i.e. users\\\\myuser\\\\documents\\\\file\n",
    "\n",
    "df = pd.read_csv(\"Your_file_goes_here\", \n",
    "                 skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** Here we need to set the correct column names for each crucial piece of information.\n",
    "- url_column_name: The column which contains the URL crawled\n",
    "- html_source_column_name: The column which contains the full extracted HTML of the page.\n",
    "- alternate_columns: A list of all the columns which contain alt tags.\n",
    "- status_code_column_name: The column which contains the status code.\n",
    "- mobile_identifier: A snippet to identify the correct alternate tag. Typically ://m. will work for 99% of cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_column_name = \"Address\"\n",
    "html_source_column_name = \"Extractor 1 1\"\n",
    "alternate_columns = [\"Extractor 2 1\", \"Extractor 2 2\"]\n",
    "status_code_column_name = \"Status Code\"\n",
    "mobile_identifier = \"://m.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** Set-up the functions that we're going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_visible_text(string):\n",
    "    '''\n",
    "    This function is run on an HTML page and returns a list of elements and any visible text.\n",
    "    \n",
    "    Returns: List\n",
    "    '''\n",
    "    if pd.notnull(string):\n",
    "        soup = BeautifulSoup(string, 'lxml')\n",
    "        texts = soup.findAll(text=True)\n",
    "        visible_text = filter(filter_visible_text, texts)\n",
    "\n",
    "        list_text = [\n",
    "            text\n",
    "            for text in visible_text \n",
    "            if len(text) > 50\n",
    "        ]\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "    return list_text\n",
    "\n",
    "\n",
    "def filter_visible_text(element):\n",
    "    '''\n",
    "    Take a beautiful soup string element and return the element if it is visible and not\n",
    "    whitespace.\n",
    "    \n",
    "    Returns: BS4 NavigableString \n",
    "    '''\n",
    "\n",
    "    if re.match(\"^\\s*$\", element):\n",
    "        return False\n",
    "        \n",
    "    return tag_visible(element)\n",
    "\n",
    "\n",
    "def tag_visible(element):\n",
    "    '''\n",
    "    This function takes a BS4 navigable string and returns any visible element. \n",
    "    \n",
    "    Returns: BS4 navigable string.\n",
    "    '''\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def pivot_crawl_md_columns(df, how_match, url_column_name, html_source_column_name):\n",
    "    '''\n",
    "    This function takes a dataframe with mobile and desktop URLs and then pivots it\n",
    "    so that mobile and desktop URLs are in columns. It can either work on path, or on \n",
    "    alternate urls.\n",
    "    \n",
    "    Returns: A dataframe.\n",
    "    '''\n",
    "\n",
    "    df_mobile = df[df[\"m_or_d\"] == \"mobile\"][[url_column_name, html_source_column_name, \"path\"]]\n",
    "        \n",
    "    if how_match == \"path\":\n",
    "        df_desktop = df[df[\"m_or_d\"] == \"desktop\"][[url_column_name, html_source_column_name, \"path\"]]\n",
    "        df_pivot = pd.merge(df_desktop, df_mobile, how=\"left\", on=\"path\")\n",
    "        \n",
    "        df_pivot.rename(columns={'Address_x':'Desktop URL', 'Address_y':'Mobile URL', 'path':'Shared Path'}, inplace=True)\n",
    "    else:\n",
    "        df_desktop = df[df[\"m_or_d\"] == \"desktop\"][[url_column_name, html_source_column_name, \"path\", \"alternate_url\"]]\n",
    "        df[\"alternate_url\"] = df.apply(lambda x: get_alternate_url(x, alternate_columns, mobile_identifier), axis=1)\n",
    "        df_mobile.rename(columns={'Address':'alternate_url'}, inplace=True)\n",
    "\n",
    "        df_pivot = pd.merge(df_desktop, df_mobile, how=\"left\", on=\"alternate_url\")\n",
    "        \n",
    "        df_pivot.drop([\"path_y\"], axis=1, inplace=True)\n",
    "        df_pivot.rename(columns={'Address':'Desktop URL', 'alternate_url':'Mobile URL', 'path_x':'Desktop Path'}, inplace=True)\n",
    "    return df_pivot\n",
    "\n",
    "\n",
    "def get_alternate_url(row, column_list, mobile_identifier):\n",
    "    '''\n",
    "    Takes a row from a dataframe and a list of columns, it will return\n",
    "    the first column that contains the mobile_identifier.\n",
    "    \n",
    "    Returns: A string\n",
    "    '''\n",
    "    \n",
    "    alt_tag_destination = \"\"\n",
    "    for column in column_list:\n",
    "        alt_tag = row[column]\n",
    "        if pd.notnull(alt_tag):\n",
    "            if mobile_identifier in alt_tag:\n",
    "                alt_tag_destination = re.search(\"href=[\\\"']([^\\\"']*)[\\\"']\", alt_tag).group(1)\n",
    "                return alt_tag_destination\n",
    "    \n",
    "    return alt_tag_destination\n",
    "\n",
    "\n",
    "def get_list_diff(row, column1, column2):\n",
    "    '''\n",
    "    Takes a row from a dataframe and two columns which contain lists and then diffs them. \n",
    "    \n",
    "    Returns: A list\n",
    "    '''\n",
    "    diff = set(row[column1]).symmetric_difference(row[column2])\n",
    "        \n",
    "    missing_from_mobile = [elem for elem in diff if elem in row[column1]]\n",
    "    \n",
    "    if len(missing_from_mobile) == 0:\n",
    "        missing_from_mobile = [\"nothing\"]\n",
    "    \n",
    "    print(\"a: {}\".format(missing_from_mobile))\n",
    "#     return [\"dom\"]\n",
    "    return missing_from_mobile\n",
    "\n",
    "\n",
    "def delete_keys_from_dict(dict_del, lst_keys):\n",
    "    '''\n",
    "    This function takes a nested python data structure of dictionaries and lists\n",
    "    and removes all keys in the provided list.\n",
    "    \n",
    "    Returns: A dictionary.\n",
    "    '''\n",
    "\n",
    "    if isinstance(dict_del, dict):\n",
    "        for k in lst_keys:\n",
    "            try:\n",
    "                del dict_del[k]\n",
    "            except KeyError:\n",
    "                pass\n",
    "        for k,v in dict_del.items():\n",
    "            if isinstance(v, dict):\n",
    "                delete_keys_from_dict(v, lst_keys)\n",
    "            elif isinstance(v, list):\n",
    "                delete_keys_from_dict(v, lst_keys)\n",
    "    elif isinstance(dict_del, list):\n",
    "        for v in dict_del:\n",
    "            delete_keys_from_dict(v, lst_keys)\n",
    "\n",
    "    return dict_del\n",
    "\n",
    "\n",
    "def set_all_dict_values_to_empty(dict_del):\n",
    "    '''\n",
    "    Sets all values in a dictionary to empty.\n",
    "    \n",
    "    Returns: A dictionary.\n",
    "    '''\n",
    "\n",
    "    if isinstance(dict_del, dict):\n",
    "        for k,v in dict_del.items():\n",
    "            if isinstance(v, dict):\n",
    "                set_all_dict_values_to_empty(v)\n",
    "            elif isinstance(v, list):\n",
    "                set_all_dict_values_to_empty(v)\n",
    "            else:\n",
    "                if k == 'value':\n",
    "                    dict_del[k] = ''\n",
    "    elif isinstance(dict_del, list):\n",
    "        for v in dict_del:\n",
    "            set_all_dict_values_to_empty(v)\n",
    "\n",
    "    return dict_del\n",
    "\n",
    "\n",
    "def clean_empty(d):\n",
    "    '''\n",
    "    This function takes a dictionary and removes alls keys where the value\n",
    "    is an empty list.\n",
    "    \n",
    "    Returns: A dictionary.\n",
    "    '''\n",
    "    if not isinstance(d, (dict, list)):\n",
    "        return d\n",
    "    if isinstance(d, list):\n",
    "        return [v for v in (clean_empty(v) for v in d) if v]\n",
    "    return {k: v for k, v in ((k, clean_empty(v)) for k, v in d.items()) if v}\n",
    "\n",
    "            \n",
    "def fetch_structured_data(string, cookie_header):\n",
    "    if pd.isnull(string):\n",
    "        return []\n",
    "    \n",
    "    headers = {'cookie':cookie_value}\n",
    "    form_payload = {\n",
    "        'url':string\n",
    "    }\n",
    "    r = requests_retry_session().post('https://search.google.com/structured-data/testing-tool/u/0/validate', \n",
    "                      headers=headers, \n",
    "                      data=form_payload, \n",
    "                      files={'set_multipart':'true'})\n",
    "    \n",
    "    # Be respectful of Structured Data Testing Tool\n",
    "    time.sleep(4)\n",
    "    \n",
    "    if r.status_code != 200:\n",
    "        print(\"Request has failed, more information below.\")\n",
    "        print(r.status_code)\n",
    "        print(r.text)\n",
    "        return \"\"\n",
    "\n",
    "    # Extract the JSON payload\n",
    "    data = json.loads(r.text[5:])\n",
    "\n",
    "    # Check request succeeded.\n",
    "    entities = data['tripleGroups']\n",
    "\n",
    "    delete_keys = [\n",
    "        'end', \n",
    "        'errors', \n",
    "        'begin', \n",
    "        'errorID', \n",
    "        'numErrors',\n",
    "        'numWarnings',\n",
    "        'numErrors',\n",
    "        'richCardPreviewState',\n",
    "        'richCardVerticalHints',\n",
    "        'ownerSet',\n",
    "        'errorsByOwner',\n",
    "        'types',\n",
    "        'warningsByOwner',\n",
    "        'numNodesWithError',\n",
    "        'numNodesWithWarning'\n",
    "    ]\n",
    "    \n",
    "    processed_sd = [clean_empty(delete_keys_from_dict(entity, delete_keys)) for entity in entities if 'nodes' in entity]\n",
    "    \n",
    "    return processed_sd\n",
    "\n",
    "\n",
    "def key_mapping(key):\n",
    "    '''\n",
    "    This takes a string and returns another string. Used to map Google SD dicts to microdata for ease of reading.\n",
    "    '''\n",
    "    if key == 'pred':\n",
    "        return 'itemprop'\n",
    "    elif key == 'typeGroup':\n",
    "        return 'itemtype'\n",
    "    elif key == 'idProperty':\n",
    "        return '@id'\n",
    "    else:\n",
    "        return key\n",
    "\n",
    "\n",
    "def change_keys(obj, convert):\n",
    "    '''\n",
    "    Recursively goes through the dictionary obj and replaces keys with the convert function.\n",
    "    Taken from: \n",
    "    https://stackoverflow.com/questions/11700705/python-recursively-replace-character-in-keys-of-nested-dictionary\n",
    "    '''\n",
    "    if isinstance(obj, (str, int, float)):\n",
    "        return obj\n",
    "    if isinstance(obj, dict):\n",
    "        new = obj.__class__()\n",
    "        for k, v in obj.items():\n",
    "            new[convert(k)] = change_keys(v, convert)\n",
    "    elif isinstance(obj, (list, set, tuple)):\n",
    "        new = obj.__class__(change_keys(v, convert) for v in obj)\n",
    "    else:\n",
    "        return obj\n",
    "    return new\n",
    "\n",
    "\n",
    "def get_json_list_diff(row, column1, column2, value_change=True):\n",
    "    '''\n",
    "    Takes a row from a dataframe and two columns which contain lists and then diffs them. \n",
    "    \n",
    "    Returns: A list\n",
    "    '''\n",
    "    structured_data_1 = row[column1]\n",
    "    structured_data_2 = row[column2]\n",
    "    \n",
    "    # https://github.com/pandas-dev/pandas/issues/14217\n",
    "    diff_sd = ['nothing']\n",
    "    for dict1 in structured_data_1:\n",
    "        is_identical = 0\n",
    "        for dict2 in structured_data_2:\n",
    "            diffed_dict = DeepDiff(dict1, dict2, ignore_order=True)\n",
    "\n",
    "            if bool(diffed_dict) is False:\n",
    "                is_identical = 1\n",
    "                break\n",
    "\n",
    "        if is_identical == 0:\n",
    "            diff_sd.append(dict1)  \n",
    "\n",
    "    return diff_sd\n",
    "\n",
    "\n",
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=5,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** Before we can do any analysis we need to get our data in a usable form. \n",
    "\n",
    "We need to change the format of our data so we have aligned each mobile page with it's desktop counterpart. We're also going to filter out any pages which aren't 200s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[df[status_code_column_name] == 200]\n",
    "df_excluded = df[df[status_code_column_name] != 200]\n",
    "df['path'] = df[url_column_name].apply(lambda x: urllib.parse.urlparse(x).path)\n",
    "df['m_or_d'] = df[url_column_name].apply(lambda x: \"mobile\" if mobile_identifier in x else \"desktop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** Here we pivot the mobile and desktop columns so they're in the same row. \n",
    "\n",
    "Then we get any text from elements which would appear by default on the page. We ignore non-standard elements like `<script>` and `<title>`.\n",
    "\n",
    "Finally once we've extracted all the differences in text we diff the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pivot = pivot_crawl_md_columns(df, \"path\", url_column_name, html_source_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pivot[\"desktop_text_list\"] = df_pivot[html_source_column_name+\"_x\"].apply(lambda x: get_visible_text(x))\n",
    "df_pivot[\"mobile_text_list\"] = df_pivot[html_source_column_name+\"_y\"].apply(lambda x: get_visible_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can't use apply because of https://github.com/pandas-dev/pandas/issues/14217\n",
    "missing_from_mobile = []\n",
    "for index, row in df_pivot.iterrows():\n",
    "    diff = set(row[\"desktop_text_list\"]).symmetric_difference(row[\"mobile_text_list\"])\n",
    "    \n",
    "    missing_from_mobile.append([elem for elem in diff if elem in row[\"desktop_text_list\"]])\n",
    "\n",
    "df_pivot[\"text_diff\"] = missing_from_mobile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** Next we need to get the structured data. We're going to use Google's Structured data testing tool because it does a huge amount of grunt work and makes some sensible design decisions.\n",
    "\n",
    "In order to use the Structured data testing tool API, you have to get an \"API\" key, which in this case is the cookie it uses for authentication. Head to the [tool](\"https://search.google.com/structured-data/testing-tool/u/0/\") and make a successful request with Chrome dev tools open.\n",
    "\n",
    "Select the request called _validate_ and copy the \"Cookie\" request header and set the value in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The cookie value should start something like: 'CONSENT=YES+GB.en+V7; SID=WEsdf9808sdflklLMPUBEPMNlk23l23a1BmNBkMpssuxM6YFHF50nxlXa1mgFw.; HSID=Asrnsdf897j;...'\n",
    "cookie_value = 'CONSENT=YES+GB.en+V7; SID=cQsdfa2fSZ....'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** Now we go out and get the structured data. Desktop and then mobile. Because this can take rather a long time we initialise tqdm which now supports pandas, a progress meter for loops or other long running operations.\n",
    "\n",
    "Important note, this structured data diff, is only capable of telling you that the objects are different, it currently won't show you what is different in them.\n",
    "\n",
    "If you want to diff with values as well then you'll need to delete the function: set_all_dict_values_to_empty on both cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tqdm_notebook().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pivot[\"desktop_sd_list\"] = df_pivot[\"Desktop URL\"].progress_apply(lambda x: set_all_dict_values_to_empty(change_keys(fetch_structured_data(x, cookie_value),key_mapping)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pivot[\"mobile_sd_list\"] = df_pivot[\"Mobile URL\"].progress_apply(lambda x: set_all_dict_values_to_empty(change_keys(fetch_structured_data(x, cookie_value),key_mapping)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** Then we diff the structured data we have pulled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_pivot['sd_diff'] = df_pivot.apply(lambda x: get_json_list_diff(x, \"desktop_sd_list\", \"mobile_sd_list\", value_change=False), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pivot[\"sd_diff\"] = df_pivot.apply(lambda x: get_json_list_diff(x, \"desktop_sd_list\", \"mobile_sd_list\", value_change=False), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Notes:** For ease of output, we now want our data stacked, where each line is one missing line of text or structured data array. We have to sack off any HTML output here or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pivot[\"num_diff_sd_objs\"] = df_pivot['sd_diff'].apply(lambda x: len(x))\n",
    "df_pivot[\"num_diff_text_strings\"] = df_pivot['text_diff'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pivot_text = df_pivot[['Desktop URL', 'Mobile URL', 'desktop_text_list', 'mobile_text_list', 'text_diff','num_diff_text_strings']]\n",
    "df_pivot_sd = df_pivot[['Desktop URL', 'Mobile URL', 'desktop_sd_list', 'mobile_sd_list', 'sd_diff','num_diff_sd_objs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stacked_text_diff = (df_pivot_text.text_diff.apply(pd.Series).stack()\n",
    "              .reset_index(level=1, drop=True)\n",
    "              .to_frame('text_diff'))\n",
    "df_pivot_text = df_pivot_text.drop(['text_diff'], axis=1).join(stacked_text_diff, how=\"left\").reset_index().drop([\"index\"], axis=1)\n",
    "df_pivot_text['category'] = 'text'\n",
    "df_pivot_text['length'] = df_pivot_text['text_diff'].apply(lambda x: len(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stacked_sd_diff = (df_pivot_sd.sd_diff.apply(pd.Series).stack()\n",
    "              .reset_index(level=1, drop=True)\n",
    "              .to_frame('sd_diff'))\n",
    "df_pivot_sd = df_pivot_sd.drop(['sd_diff'], axis=1).join(stacked_sd_diff, how=\"left\").reset_index().drop([\"index\"], axis=1)\n",
    "df_pivot_sd['category'] = 'structured_data'\n",
    "df_pivot_sd['length'] = df_pivot_sd['sd_diff'].apply(lambda x: len(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** Finally we join the stacked dataframes with a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = pd.concat([df_pivot_sd,df_pivot_text])\n",
    "output.drop([\"desktop_sd_list\", \"mobile_sd_list\", \"desktop_text_list\", \"mobile_text_list\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.to_csv(\"structured_data_schema_output.csv\", sep=',', quoting=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
